# Environment Settings
gpu_id: '0'
use_gpu: True
seed: 212
reproducibility: True
data_path: 'dataset/'
checkpoint_dir: 'saved'

# Dataset Settings
load_col:
    inter: [user_id, item_id, rating, timestamp]
    item: ['item_id','title','sales_rank','price','brand','categories','sales_type']
MAX_ITEM_LIST_LENGTH: 50
USER_ID_FIELD: user_id
ITEM_ID_FIELD: item_id
RATING_FIELD: rating
TIME_FIELD: timestamp

# Filtering
val_interval: ~
filter_inter_by_user_or_item: True
user_inter_num_interval: "[5,inf)"
item_inter_num_interval: "[5,inf)"

# Training Settings
epochs: 300
train_batch_size: 2048
learner: adam
learning_rate: 0.001
eval_step: 1
stopping_step: 20
clip_grad_norm: ~
weight_decay: 0.0
loss_decimal_place: 4
loss_type: 'CE' 
train_neg_sample_args: ~ 

# # Model Settings
n_layers: 2                     
n_heads: 4                     
hidden_size: 32                 
reduction_ratio: 2
learning_rate: 0.001
kernel_size: 3
hidden_dropout_prob: 0.5        
attn_dropout_prob: 0.5        
hidden_act: 'gelu'              


# Evaluation Settings
eval_args:
  split: {'LS': 'valid_and_test'}
  group_by: user
  order: TO
  mode: uni100
repeatable: True
metrics: ["Recall","NDCG"]
topk: [1,5,10,20]
valid_metric: Recall@10
valid_metric_bigger: True
eval_batch_size: 4096
metric_decimal_place: 4