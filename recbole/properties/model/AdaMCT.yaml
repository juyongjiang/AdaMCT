n_layers: 2                     # (int) The number of transformer layers in transformer encoder. 
n_heads: 4                      # (int) The number of attention heads for multi-head attention layer. 
hidden_size: 64                 # (int) The number of features in the hidden state. 
reduction_ratio: 2              # (int) The number of reduction ratio in the SEAtt. 
kernel_size: 3                  # (int) The size of kernel in the convolutional layer.
hidden_dropout_prob: 0.5        # (float) The probability of an element to be zeroed.
attn_dropout_prob: 0.5          # (float) The probability of an attention score to be zeroed.
hidden_act: 'gelu'              # (str) The activation function in convolution layer.
layer_norm_eps: 1e-12           # (float) A value added to the denominator for numerical stability. 
initializer_range: 0.02         # (float) The standard deviation for normal initialization.
loss_type: 'CE'                 # (str) The type of loss function. Range in ['BPR', 'CE'].
train_neg_sample_args: ~        # ï½ž denotes None
